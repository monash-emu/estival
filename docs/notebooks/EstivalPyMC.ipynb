{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821820a8",
   "metadata": {},
   "source": [
    "# Estival/PyMC\n",
    "\n",
    "In this notebook, we will build a BayesianCompartmentalModel, and calibrate it using PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following to install in colab\n",
    "# Note that you need to restart your colab environment after installing - just click the 'Restart' button\n",
    "\n",
    "# !pip uninstall numba -y\n",
    "# !pip uninstall librosa -y\n",
    "# !pip install estival==0.4.4 numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required for pymc parallel evaluation in notebooks\n",
    "\n",
    "import multiprocessing as mp\n",
    "import platform\n",
    "\n",
    "if platform.system() != \"Windows\":\n",
    "    mp.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import summer2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d790a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summer2.extras import test_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = test_models.sir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0844c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "defp = m.get_default_parameters()\n",
    "defp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bde71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.run({\"contact_rate\": 0.5, \"recovery_rate\": 0.4})\n",
    "do_def = m.get_derived_outputs_df()\n",
    "obs_clean = do_def[\"incidence\"].iloc[0:50]\n",
    "obs_noisy = obs_clean * np.exp(np.random.normal(0.0,0.2,len(obs_clean)))\n",
    "obs_clean.plot()\n",
    "obs_noisy.plot(style='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following imports are the 'building blocks' of estival models\n",
    "\n",
    "# Targets represent data we are trying to fit to\n",
    "from estival import targets as est\n",
    "\n",
    "# We specify parameters using (Bayesian) priors\n",
    "from estival import priors as esp\n",
    "\n",
    "# Finally we combine these with our summer2 model in a BayesianCompartmentalModel (BCM)\n",
    "from estival.model import BayesianCompartmentalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24785c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Truncated normal target with a free dispersion parameter\n",
    "targets = [\n",
    "    est.TruncatedNormalTarget(\"incidence\", obs_noisy, (0.0,np.inf),\n",
    "        esp.UniformPrior(\"incidence_dispersion\",(0.1, obs_noisy.max()*0.1)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29170d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform priors over our 2 model parameters\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", (0.01,1.0)),\n",
    "    esp.TruncNormalPrior(\"recovery_rate\", 0.5, 0.2, (0.01,1.0)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BayesianCompartmentalModel class is the primary entry point to all optimization and calibration\n",
    "# methods in estival\n",
    "# It takes a CompartmentalModel object, default parameters, priors, and targets\n",
    "# The default parameters will be used as fixed values when no prior is specified for a given parameter\n",
    "\n",
    "bcm = BayesianCompartmentalModel(m, defp, priors, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estival.wrappers import pymc as epm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf40b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    # This is all you need - a single call to use_model\n",
    "    variables = epm.use_model(bcm)\n",
    "    \n",
    "    # The log-posterior value can also be output, but may incur additional overhead\n",
    "    # Use jacobian=False to get the unwarped value (ie just the 'native' density of the priors\n",
    "    # without transformation correction factors)\n",
    "    # pm.Deterministic(\"logp\", model.logp(jacobian=False))\n",
    "    \n",
    "    # Now call a sampler using the variables from use_model\n",
    "    # In this case we use the Differential Evolution Metropolis sampler\n",
    "    # See the PyMC docs for more details\n",
    "    idata = pm.sample(step=[pm.DEMetropolis(variables)], draws=4000, tune=0,cores=4,chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063066c",
   "metadata": {},
   "source": [
    "## Using arviz to examine outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48174240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - select some subset out of the resulting trace - useful if\n",
    "# you require additional burnin\n",
    "# subset = idata.sel(draw=slice(500, None), groups=\"posterior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, figsize=(16,3.2*len(idata.posterior)),compact=False);#, lines=[(\"m\", {}, mtrue), (\"c\", {}, ctrue)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc584ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(idata);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bafdc3",
   "metadata": {},
   "source": [
    "### Obtaining likelihood\n",
    "\n",
    "It is frequently useful to examine the (log)likelihood values of the samples in addition to their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb79fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood_extras_for_idata will compute log likelihood, prior and posterior values,\n",
    "# as well as invidual likelihood components for each target\n",
    "\n",
    "from estival.sampling.tools import likelihood_extras_for_idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13becd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_df = likelihood_extras_for_idata(idata, bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the performance of chains over time\n",
    "\n",
    "ldf_pivot = likelihood_df.reset_index(level=\"chain\").pivot(columns=[\"chain\"])\n",
    "\n",
    "ldf_pivot[\"logposterior\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort this DataFrame by logposterior to obtain the MAP index\n",
    "ldf_sorted = likelihood_df.sort_values(by=\"logposterior\",ascending=False)\n",
    "\n",
    "# Extract the parameters from the calibration samples\n",
    "map_params = idata.posterior.to_dataframe().loc[ldf_sorted.index[0]].to_dict()\n",
    "\n",
    "map_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4607422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, this is exactly the value output from the original BCM\n",
    "bcm.loglikelihood(**map_params), ldf_sorted.iloc[0][\"loglikelihood\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model with these parameters\n",
    "map_res = bcm.run(map_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec989ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and plot some results\n",
    "variable = \"incidence\"\n",
    "\n",
    "pd.Series(map_res.derived_outputs[variable]).plot(title = f\"{variable} (MLE)\")\n",
    "bcm.targets[variable].data.plot(style='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0be80a",
   "metadata": {},
   "source": [
    "#### Uncertainty sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the arviz extract method to obtain some samples, then convert to a DataFrame\n",
    "sample_idata = az.extract(idata, num_samples = 400)\n",
    "samples_df = sample_idata.to_dataframe().drop(columns=[\"chain\",\"draw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d15655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use estivals parallel tools to run the model evaluations\n",
    "\n",
    "from estival.utils.parallel import map_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function captures our bcm from the main namespace to pass into map_parallel\n",
    "# Using this idiom in closures/factory functions is typical\n",
    "def run_sample(idx_sample):\n",
    "    idx, params = idx_sample\n",
    "    return idx, bcm.run(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the samples through our BCM using the above function\n",
    "# map_parallel takes a function and an iterable as input\n",
    "\n",
    "# We use 4 workers here, default is cpu_count/2 (assumes hyperthreading)\n",
    "sample_res = map_parallel(run_sample, samples_df.iterrows(), n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use xarray for this step; aside from computing things very quickly, it's useful\n",
    "# to persist the run results to netcdf/zarr etc\n",
    "\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2500ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataArray out of our results, then assign coords for indexing\n",
    "xres = xr.DataArray(np.stack([r.derived_outputs for idx, r in sample_res]), \n",
    "                    dims=[\"sample\",\"time\",\"variable\"])\n",
    "xres = xres.assign_coords(sample=sample_idata.coords[\"sample\"], \n",
    "                          time=map_res.derived_outputs.index, variable=map_res.derived_outputs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59853b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some quantiles to calculate\n",
    "quantiles = (0.01,0.05,0.25,0.5,0.75,0.95,0.99)\n",
    "\n",
    "# Generate a new DataArray containing the quantiles\n",
    "xquantiles = xres.quantile(quantiles,dim=[\"sample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecdf4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract these values to a pandas DataFrame for ease of plotting\n",
    "\n",
    "uncertainty_df = xquantiles.to_dataframe(name=\"value\").reset_index().set_index(\"time\").pivot(columns=(\"variable\",\"quantile\"))[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"incidence\"\n",
    "\n",
    "fig = uncertainty_df[variable].plot(title=variable,alpha=0.7)\n",
    "pd.Series(map_res.derived_outputs[variable]).plot(style='--')\n",
    "bcm.targets[variable].data.plot(style='.',color=\"black\", ms=3, alpha=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9032fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
